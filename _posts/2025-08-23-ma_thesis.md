---
title: "Task-Specific Machine Translation with LLMs (Siemens AG)"
date: 2025-06-05 12:00:00 +0200
categories: [Research, Thesis]
tags: [machine translation, prompt engineering, rag, fine-tuning, evaluation]
description: "Masterâ€™s thesis at Siemens AG: Built domain-specific MT pipeline with LLMs, improving translation quality and achieving 64% cost reduction compared to APIs."
author: aaron
image:
  path: /assets/img/thesis.jpg
  alt: Pipeline
pin: true
---

### ðŸŽ¯ Goals
 
**Develop and evaluate task-specific machine translation (MT) solutions** for **technical documentation (DEâ†”EN)** using **Large Language Models (LLMs)**.

Focus:
1. **Performance improvement** â€” achieving translations closer to human quality  
2. **Cost reduction** â€” minimizing expenses compared to proprietary API solutions

---

### ðŸš€ Key Contributions

1. **Preprocessing** - engineered a scalable data pipeline to clean, align, and curate high-quality parallel corpora.
2. **Prompting approaches** â€“ explored prompt engineering strategies (few-shot, RAG). 
3. **Fine-tuning methods** â€“ experimented with full fine-tuning and parameter-efficient methods (LoRA/QLoRA).  
4. **Model selection** â€“ benchmarked open-source (LLaMA-3.1) against proprietary APIs (GPT-4o-mini).  
5. **Evaluation** â€“ designed a robust assessment pipeline combining automatic metrics (e.g. BLEU, COMET, MetricX-24) with human annotation.

---

### ðŸ“Š Results

- **Higher translation quality** â€” fine-tuned LLaMA-3.1 showed improved COMET and MetricX-24 scores, outperforming commercial APIs. 
- **Up to 64% cost reduction** while surpassing proprietary API performance.
- **Validated evaluation** â€” strong alignment between COMET/MetricX-24 and human judgments confirmed the pipelineâ€™s reliability for scalable, real-world deployment.

---

### ðŸ›  Tech Stack

| Area               | Tools & Frameworks                                            |
| ------------------ | ------------------------------------------------------------- |
| **Languages**      | Python (3.11), Bash                                           |
| **ML Frameworks**  | PyTorch, Hugging Face Transformers, Langchain, Unsloth, FAISS |
| **Infrastructure** | Azure AI, Azure Blob Storage, GitLab CI, Docker               |
| **Models**         | GPT-4o, GPT-4o-mini, Llama 3.1                                |
| **Data**           | Large Translation Memories (TMx)                              |

> **Open-source fine-tuning achieved state-of-the-art quality at 64% lower cost compared to proprietary APIs.**
{: .prompt-tip }
