---
title: "TriviaQA: Question Answering with LLMs"
date: 2023-12-22 12:00:00 +0200
categories: [Research, Project]
tags: [question-answering, rag, fine-tuning, prompt engineering, evaluation]
description: "Applied prompting, RAG, and fine-tuning with LLMs on the TriviaQA dataset to improve accuracy and factual consistency."
author: aaron
image:
  path: /assets/img/triviaqa.jpg
  alt: Pipeline
pin: false
---

### ðŸŽ¯ Goals

**Develop and evaluate LLM-based approaches for open-domain Question Answering** using the **TriviaQA dataset** with Wikipedia context.

Focus:
1. **Accuracy** â€” maximize correct answers to open questions
2. **Context fidelity** â€” ensure generated answers remain faithful to source passages
3. **Model efficiency** â€” compare trade-offs between prompting, RAG, and fine-tuning

---

### ðŸš€ Key Contributions

1. **Retrieval-Augmented Generation (RAG)** â€” integrated Wikipedia passages via retrieval to improve factual grounding.
2. **Fine-Tuning** â€” applied full fine-tuning on TriviaQA for improved answer consistency.
3. **Evaluation** â€” measured performance using **Exact Match (EM), F1 score**, and qualitative analysis of context adherence.

---

### ðŸ“Š Results

- **Fine-tuned models** achieved the highest **Exact Match (EM)** and **F1 scores**, delivering the most accurate answers overall.  
- **RAG-based prompting** improved factual grounding and context fidelity compared to plain prompting, though it did not surpass fine-tuned models in raw accuracy.
- Demonstrated that **prompt engineering alone is insufficient** for open-domain QA; retrieval and adaptation are critical for robust performance.

---

### ðŸ›  Tech Stack

| Area               | Tools & Frameworks                                                                   |
| ------------------ | ------------------------------------------------------------------------------------ |
| **Languages**      | Python, Bash                                                                         |
| **ML Frameworks**  | PyTorch, Hugging Face Transformers, LangChain, Haystack                              |
| **Infrastructure** | Google Colab, Slurm                                                                  |
| **Models**         | Google T5, FLAN-T5                                                                   |
| **Data**           | TriviaQA (Wikipedia context)                                                         |
| **Repository**     | [https://github.com/aaronkossler/triviaqa](https://github.com/aaronkossler/triviaqa) |


---

> **Takeaway:** Retrieval-Augmented Generation significantly boosts factual consistency in open-domain QA tasks, while fine-tuning offers additional gains at the cost of complexity.
{: .prompt-tip }
